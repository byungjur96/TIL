# 1. GET STARTED

### 1. Quick Tour

#### Transformers Library

- NLU 태스크(감성 분석)와 NLG 태스크(문장 생성, 번역)를 위한 사전 학습 모델을 다운 받는다.
- 추론 과정에서 사전 학습 모델을 사용할 때 pipeline API 을 손쉽게 활용할 수 있음.

#### `pipeline()`

- 특정 태스크에 사전 학습을 사용하기에 가장 쉬운 방법

- 제공되는 태스크들
  1. **Sentiment Analysis** : 텍스트의 긍정/부정 판별.

  2. **Text Generation** : 주어진 promt에 대해 모델이 뒷 내용을 완성.

     ```python
     # 감성 분석의 예시
     from transformers import pipeline
     classifier = pipeline('sentiment-analysis')
     # 여러 문장에 대해 실행하고 싶은 경우 list 형태로 넘겨주면 됨
     classifier('We are very happy to study Transformers library.')
     # OUTPUT : [{'label': 'POSITIVE', 'score': 0.9998047947883606}]
     ```

  3. **NER** : 각 단어가 어떤 객체(entity)를 나타내는지 라벨링. (person, place, etc.)

  4. **QA** : 'context'와 질문에 대해 'context'로부터 대답을 추출.

  5. **Fillling Masked Text** : `[MASK]` 로 가려진 단어를 채우기.

  6. **Summarization** : 긴 텍스트에 대한 요약문 생성

  7. **Translation** : 다른 언어로 번역.

  8. **Feature Extraction** : 텍스트를 tensor로 표현.

> ⚠️ `IProgress not found. Please update jupyter and ipywidgets.`
>
> - `jupyter`의 버전에 문제가 있을 경우 `pipeline()`을 불러오는데 오류가 생김.
> - 가상 환경 내에서 `pip`을 통해 `jupyter`와 `ipywidgets`의 버전을 확인해줘야 함.

- 기본적으로 `distilBERT` 기반의 모델을 다운로드 함. (`distilbert-base-uncased-finetuned-sst-2-english`)

- `pipeline()` 메서드에 model을 paremeter로 전달해줄 수 있음 (Local Folder를 사용할 수도 있음)

  ```python
  classifier = pipeline('sentiment-analysis', model="nlptown/bert-base-multilingual-uncased-sentiment")
  ```

#### 사전학습 모델 이용하기

- `AutoTokenizer` : 모델과 관련된 tokenizer를 다운로드하는 Class.

- `AutoModelForSequenceClassification` : 모델을 다운로드하는 Class. (PyTorch 기준)

- 각 class의 `from_pretrained()` 메서드를 이용하여 다운로드할 수 있음.

  ```python
  from transformers import AutoTokenizer, AutoModelForSequenceClassification
  model_name = "distilbert-base-uncased-finetuned-sst-2-english"
  pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
  tokenizer = AutoTokenizer.from_pretrained(model_name)
  ```

- `tokenizer` ?

  1. 주어진 text를 *token*(단어, 구두점, ...)으로 나눔

  2. *vocab*을 이용해서 token을 숫자로 바꿈 (tensor로 만들어서 모델에서 이용할 수 있도록)

     → `from_pretrained()` 를 이용하여 인스턴스화함 (❗️사전 학습 시 사용한 vocab과 동일해야!)

  3. 'input_id'와 'attention_mask'의 list를 갖고 있는 dictionary를 return.

     (문장 시작 id는 101, '.'은 1012, 문장 끝은 102)

     ```python
     inputs = tokenizer("We are very happy to study Transformers library.")
     #  {
     #    'input_ids': [101, 2057, 2024, 2200, 3407, 2000, 2817, 19081, 3075, 1012, 102], 
     #    'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
     #  }
     ```

  4. 만약 model에 batch로 보내고 싶은 경우 동일한 길이를 같도록 padding을 추가하거나 모델이 수용할 수 있는 최대 길이로 자르고(truncate) tensor를 반환함.

     ```python
     pt_batch = tokenizer(
         ["We are very happy to show you the 🤗 Transformers library.", 
          "We hope you don't hate it."],
       	# padding 적용 여부
         padding=True,
         truncation=True,
       	# truncation을 사용할 경우 max_length를 설정해줘야 함.
       	max_length=10,
       	# PyTorch의 경우 pt, TensorFlow의 경우 tf
         return_tensors="pt"
     )
     ```

  5. tokenizer를 통해 전처리한 input을 모델에 직접 전달하면 output을 return 받을 수 있음.

     ⚠️ 모든 output은 tuple로 return됨.

     ⚠️ 마지막 activation function 이전의 activation을 return함. (loss 함수로 값들이 섞이는 것을 막기 위해)

  6. 파인튜닝을 완료하면 `save_retrained()` 함수를 통해 모델을 저장.

     ```python
     tokenizer.save_pretrained(save_directory)
     model.save_pretrained(save_directory)
     ```

  7. `from_pretrained()` 함수를 통해 저장한 모델을 불러옴.

     ```python
     # PyTorch
     tokenizer = AutoTokenizer.from_pretrained(save_directory)
     model = TFAutoModel.from_pretrained(save_directory, from_pt=True)
     
     # TensorFlow
     tokenizer = AutoTokenizer.from_pretrained(save_directory)
     model = AutoModel.from_pretrained(save_directory, from_tf=True)
     ```

     > `output_hidden_states`, `output_attentions`을 통해 hidden states와 weights를 반환할 수 있음.

- 각 아키텍쳐에 해당하는 configuration을 수정하여 hidden dimension, dropout rate 등을 변경할 수 있음.

  ⚠️ 모델의 아키텍쳐를 수정하면 처음부터 다시 학습을 진행해야 함.

  ⚠️ 모델의 head 부분(ex. 라벨 개수)을 수정하면 사전 학습된 모델을 사용할 수 있음.

  ```python
  from transformers import DistilBertConfig, DistilBertTokenizer, DistilBertForSequenceClassification
  
  model_name = "distilbert-base-uncased"
  model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=10)
  tokenizer = DistilBertTokenizer.from_pretrained(model_name)
  ```

  

### 2. Installation

- `pip` 을 통해서 직접 설치하거나 `git clone` 을 통해서 설치 가능하다.

  1. ```bash
     pip install transformers
     ```

  2. ```bash
     git clone https://github.com/huggingface/transformers.git
     cd transformers
     pip install -e .
     ```

- 사전 학습 모델들은 로컬에 다운로드되고 캐싱됨.
- 특별한 환경 변수 설정을 하지 않으면 cache data는 `~/.cache/torch/transformers/` 에 설치됨.
  - `cache_dir=...` 로 디렉토리 직접 지정해 줄 수 있음.
  - Shell 환경 변수의 `TRANSFORMERS_CACHE` 를 통해 default 디렉토리를 지정해 줄 수 있음.
- **Swift-coreml-transformers** ([link]("https://github.com/huggingface/swift-coreml-transformers"))
  - iOS 기기에서 실행 가능한 CoreML 모델들.
  - `GPT-2` , `DistilGPT-2` , `BERT` , `DistilBERT` 등의 모델들을 포함하고 있음



### 3. Philosophy

- **Transformers의 목표 대상**

  - 대규모 transformers 모델을 사용/학습/확장하기 위한 NLP 연구자/학생
  - 모델을 파인튜닝하거나 프로덕트를 제공하기 위한 개업자(practitioners)
  - NLP 태스크를 해결하기 위해 사전 학습된 모델을 다운받아서 사용하고 싶은 공학자들

- **Library의 목표**

  1. 최대한 쉽고 빠르게 이용하기

  2. 가능한 원래 모델과 유사한 성능을 보여줄 수 있는 SOTA 모델을 제공하기

  3. 모델의 내부를 최대한 일관적으로 드러내기

  4. 이 모델들을 파인튜닝/연구하기 위해 주관적으로 선택한 유망한 툴들을 포함하기

  5. PyTorch와 TensorFlow 간의 스위칭을 용이하게 하기

     (1개의 프레임워크에서 학습하고 다른 모델에서 추론하기)

- **Main Concepts**

  1. Model Class : `Transformers` 에서 제공하는 사전 학습된 weight와 이용 가능한 PyTorch/Keras 모델

     ex. `BertModel`

  2. Configuration Class : 모델을 만들기 위해 필요한 모든 parameter를 저장

     ex. `BertConfig`

  3. Tokenizer Class : 각 모델을 위한 단어를 저장. 문장을 token embedding으로 encoding/decoding하기위한 메소드.

     ex. `BertTokenizer`

  - `from_pretrained()`를 통해 라이브러리나 로컬의 모델/configure/tokenizer를 인스턴스화 할 수 있음.
  - `save_pretrained()`를 통해 모델/configure/tokenizer를 로컬에 저장할 수 있음.



### 4. Glossary (용어 사전)

#### General Terms

- NLG
- NLP
- NLU
- Autoencoding Models
- Autoregressive Models
- CLM
- MLM
- Multimodal
- Pretrained Model
- RNN
- seq2seq
- token

#### Model Inputs

- Input IDs
- Attention Mask
- Token Type IDs
- Position IDs
- Feed Forward Chunking