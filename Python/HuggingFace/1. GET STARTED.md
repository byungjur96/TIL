# 1. GET STARTED

### 1. Quick Tour

#### Transformers Library

- NLU íƒœìŠ¤í¬(ê°ì„± ë¶„ì„)ì™€ NLG íƒœìŠ¤í¬(ë¬¸ì¥ ìƒì„±, ë²ˆì—­)ë¥¼ ìœ„í•œ ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì„ ë‹¤ìš´ ë°›ëŠ”ë‹¤.
- ì¶”ë¡  ê³¼ì •ì—ì„œ ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì„ ì‚¬ìš©í•  ë•Œ pipeline API ì„ ì†ì‰½ê²Œ í™œìš©í•  ìˆ˜ ìˆìŒ.

#### `pipeline()`

- íŠ¹ì • íƒœìŠ¤í¬ì— ì‚¬ì „ í•™ìŠµì„ ì‚¬ìš©í•˜ê¸°ì— ê°€ì¥ ì‰¬ìš´ ë°©ë²•

- ì œê³µë˜ëŠ” íƒœìŠ¤í¬ë“¤
  1. **Sentiment Analysis** : í…ìŠ¤íŠ¸ì˜ ê¸ì •/ë¶€ì • íŒë³„.

  2. **Text Generation** : ì£¼ì–´ì§„ promtì— ëŒ€í•´ ëª¨ë¸ì´ ë’· ë‚´ìš©ì„ ì™„ì„±.

     ```python
     # ê°ì„± ë¶„ì„ì˜ ì˜ˆì‹œ
     from transformers import pipeline
     classifier = pipeline('sentiment-analysis')
     # ì—¬ëŸ¬ ë¬¸ì¥ì— ëŒ€í•´ ì‹¤í–‰í•˜ê³  ì‹¶ì€ ê²½ìš° list í˜•íƒœë¡œ ë„˜ê²¨ì£¼ë©´ ë¨
     classifier('We are very happy to study Transformers library.')
     # OUTPUT : [{'label': 'POSITIVE', 'score': 0.9998047947883606}]
     ```

  3. **NER** : ê° ë‹¨ì–´ê°€ ì–´ë–¤ ê°ì²´(entity)ë¥¼ ë‚˜íƒ€ë‚´ëŠ”ì§€ ë¼ë²¨ë§. (person, place, etc.)

  4. **QA** : 'context'ì™€ ì§ˆë¬¸ì— ëŒ€í•´ 'context'ë¡œë¶€í„° ëŒ€ë‹µì„ ì¶”ì¶œ.

  5. **Fillling Masked Text** : `[MASK]` ë¡œ ê°€ë ¤ì§„ ë‹¨ì–´ë¥¼ ì±„ìš°ê¸°.

  6. **Summarization** : ê¸´ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ìš”ì•½ë¬¸ ìƒì„±

  7. **Translation** : ë‹¤ë¥¸ ì–¸ì–´ë¡œ ë²ˆì—­.

  8. **Feature Extraction** : í…ìŠ¤íŠ¸ë¥¼ tensorë¡œ í‘œí˜„.

> âš ï¸ `IProgress not found. Please update jupyter and ipywidgets.`
>
> - `jupyter`ì˜ ë²„ì „ì— ë¬¸ì œê°€ ìˆì„ ê²½ìš° `pipeline()`ì„ ë¶ˆëŸ¬ì˜¤ëŠ”ë° ì˜¤ë¥˜ê°€ ìƒê¹€.
> - ê°€ìƒ í™˜ê²½ ë‚´ì—ì„œ `pip`ì„ í†µí•´ `jupyter`ì™€ `ipywidgets`ì˜ ë²„ì „ì„ í™•ì¸í•´ì¤˜ì•¼ í•¨.

- ê¸°ë³¸ì ìœ¼ë¡œ `distilBERT` ê¸°ë°˜ì˜ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œ í•¨. (`distilbert-base-uncased-finetuned-sst-2-english`)

- `pipeline()` ë©”ì„œë“œì— modelì„ paremeterë¡œ ì „ë‹¬í•´ì¤„ ìˆ˜ ìˆìŒ (Local Folderë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŒ)

  ```python
  classifier = pipeline('sentiment-analysis', model="nlptown/bert-base-multilingual-uncased-sentiment")
  ```

#### ì‚¬ì „í•™ìŠµ ëª¨ë¸ ì´ìš©í•˜ê¸°

- `AutoTokenizer` : ëª¨ë¸ê³¼ ê´€ë ¨ëœ tokenizerë¥¼ ë‹¤ìš´ë¡œë“œí•˜ëŠ” Class.

- `AutoModelForSequenceClassification` : ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ëŠ” Class. (PyTorch ê¸°ì¤€)

- ê° classì˜ `from_pretrained()` ë©”ì„œë“œë¥¼ ì´ìš©í•˜ì—¬ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŒ.

  ```python
  from transformers import AutoTokenizer, AutoModelForSequenceClassification
  model_name = "distilbert-base-uncased-finetuned-sst-2-english"
  pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
  tokenizer = AutoTokenizer.from_pretrained(model_name)
  ```

- `tokenizer` ?

  1. ì£¼ì–´ì§„ textë¥¼ *token*(ë‹¨ì–´, êµ¬ë‘ì , ...)ìœ¼ë¡œ ë‚˜ëˆ”

  2. *vocab*ì„ ì´ìš©í•´ì„œ tokenì„ ìˆ«ìë¡œ ë°”ê¿ˆ (tensorë¡œ ë§Œë“¤ì–´ì„œ ëª¨ë¸ì—ì„œ ì´ìš©í•  ìˆ˜ ìˆë„ë¡)

     â†’ `from_pretrained()` ë¥¼ ì´ìš©í•˜ì—¬ ì¸ìŠ¤í„´ìŠ¤í™”í•¨ (â—ï¸ì‚¬ì „ í•™ìŠµ ì‹œ ì‚¬ìš©í•œ vocabê³¼ ë™ì¼í•´ì•¼!)

  3. 'input_id'ì™€ 'attention_mask'ì˜ listë¥¼ ê°–ê³  ìˆëŠ” dictionaryë¥¼ return.

     (ë¬¸ì¥ ì‹œì‘ idëŠ” 101, '.'ì€ 1012, ë¬¸ì¥ ëì€ 102)

     ```python
     inputs = tokenizer("We are very happy to study Transformers library.")
     #  {
     #    'input_ids': [101, 2057, 2024, 2200, 3407, 2000, 2817, 19081, 3075, 1012, 102], 
     #    'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
     #  }
     ```

  4. ë§Œì•½ modelì— batchë¡œ ë³´ë‚´ê³  ì‹¶ì€ ê²½ìš° ë™ì¼í•œ ê¸¸ì´ë¥¼ ê°™ë„ë¡ paddingì„ ì¶”ê°€í•˜ê±°ë‚˜ ëª¨ë¸ì´ ìˆ˜ìš©í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ ê¸¸ì´ë¡œ ìë¥´ê³ (truncate) tensorë¥¼ ë°˜í™˜í•¨.

     ```python
     pt_batch = tokenizer(
         ["We are very happy to show you the ğŸ¤— Transformers library.", 
          "We hope you don't hate it."],
       	# padding ì ìš© ì—¬ë¶€
         padding=True,
         truncation=True,
       	# truncationì„ ì‚¬ìš©í•  ê²½ìš° max_lengthë¥¼ ì„¤ì •í•´ì¤˜ì•¼ í•¨.
       	max_length=10,
       	# PyTorchì˜ ê²½ìš° pt, TensorFlowì˜ ê²½ìš° tf
         return_tensors="pt"
     )
     ```

  5. tokenizerë¥¼ í†µí•´ ì „ì²˜ë¦¬í•œ inputì„ ëª¨ë¸ì— ì§ì ‘ ì „ë‹¬í•˜ë©´ outputì„ return ë°›ì„ ìˆ˜ ìˆìŒ.

     âš ï¸ ëª¨ë“  outputì€ tupleë¡œ returnë¨.

     âš ï¸ ë§ˆì§€ë§‰ activation function ì´ì „ì˜ activationì„ returní•¨. (loss í•¨ìˆ˜ë¡œ ê°’ë“¤ì´ ì„ì´ëŠ” ê²ƒì„ ë§‰ê¸° ìœ„í•´)

  6. íŒŒì¸íŠœë‹ì„ ì™„ë£Œí•˜ë©´ `save_retrained()` í•¨ìˆ˜ë¥¼ í†µí•´ ëª¨ë¸ì„ ì €ì¥.

     ```python
     tokenizer.save_pretrained(save_directory)
     model.save_pretrained(save_directory)
     ```

  7. `from_pretrained()` í•¨ìˆ˜ë¥¼ í†µí•´ ì €ì¥í•œ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜´.

     ```python
     # PyTorch
     tokenizer = AutoTokenizer.from_pretrained(save_directory)
     model = TFAutoModel.from_pretrained(save_directory, from_pt=True)
     
     # TensorFlow
     tokenizer = AutoTokenizer.from_pretrained(save_directory)
     model = AutoModel.from_pretrained(save_directory, from_tf=True)
     ```

     > `output_hidden_states`, `output_attentions`ì„ í†µí•´ hidden statesì™€ weightsë¥¼ ë°˜í™˜í•  ìˆ˜ ìˆìŒ.

- ê° ì•„í‚¤í…ì³ì— í•´ë‹¹í•˜ëŠ” configurationì„ ìˆ˜ì •í•˜ì—¬ hidden dimension, dropout rate ë“±ì„ ë³€ê²½í•  ìˆ˜ ìˆìŒ.

  âš ï¸ ëª¨ë¸ì˜ ì•„í‚¤í…ì³ë¥¼ ìˆ˜ì •í•˜ë©´ ì²˜ìŒë¶€í„° ë‹¤ì‹œ í•™ìŠµì„ ì§„í–‰í•´ì•¼ í•¨.

  âš ï¸ ëª¨ë¸ì˜ head ë¶€ë¶„(ex. ë¼ë²¨ ê°œìˆ˜)ì„ ìˆ˜ì •í•˜ë©´ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ.

  ```python
  from transformers import DistilBertConfig, DistilBertTokenizer, DistilBertForSequenceClassification
  
  model_name = "distilbert-base-uncased"
  model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=10)
  tokenizer = DistilBertTokenizer.from_pretrained(model_name)
  ```

  

### 2. Installation

- `pip` ì„ í†µí•´ì„œ ì§ì ‘ ì„¤ì¹˜í•˜ê±°ë‚˜ `git clone` ì„ í†µí•´ì„œ ì„¤ì¹˜ ê°€ëŠ¥í•˜ë‹¤.

  1. ```bash
     pip install transformers
     ```

  2. ```bash
     git clone https://github.com/huggingface/transformers.git
     cd transformers
     pip install -e .
     ```

- ì‚¬ì „ í•™ìŠµ ëª¨ë¸ë“¤ì€ ë¡œì»¬ì— ë‹¤ìš´ë¡œë“œë˜ê³  ìºì‹±ë¨.
- íŠ¹ë³„í•œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •ì„ í•˜ì§€ ì•Šìœ¼ë©´ cache dataëŠ” `~/.cache/torch/transformers/` ì— ì„¤ì¹˜ë¨.
  - `cache_dir=...` ë¡œ ë””ë ‰í† ë¦¬ ì§ì ‘ ì§€ì •í•´ ì¤„ ìˆ˜ ìˆìŒ.
  - Shell í™˜ê²½ ë³€ìˆ˜ì˜ `TRANSFORMERS_CACHE` ë¥¼ í†µí•´ default ë””ë ‰í† ë¦¬ë¥¼ ì§€ì •í•´ ì¤„ ìˆ˜ ìˆìŒ.
- **Swift-coreml-transformers** ([link]("https://github.com/huggingface/swift-coreml-transformers"))
  - iOS ê¸°ê¸°ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•œ CoreML ëª¨ë¸ë“¤.
  - `GPT-2` , `DistilGPT-2` , `BERT` , `DistilBERT` ë“±ì˜ ëª¨ë¸ë“¤ì„ í¬í•¨í•˜ê³  ìˆìŒ



### 3. Philosophy

- **Transformersì˜ ëª©í‘œ ëŒ€ìƒ**

  - ëŒ€ê·œëª¨ transformers ëª¨ë¸ì„ ì‚¬ìš©/í•™ìŠµ/í™•ì¥í•˜ê¸° ìœ„í•œ NLP ì—°êµ¬ì/í•™ìƒ
  - ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ê±°ë‚˜ í”„ë¡œë•íŠ¸ë¥¼ ì œê³µí•˜ê¸° ìœ„í•œ ê°œì—…ì(practitioners)
  - NLP íƒœìŠ¤í¬ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ë‹¤ìš´ë°›ì•„ì„œ ì‚¬ìš©í•˜ê³  ì‹¶ì€ ê³µí•™ìë“¤

- **Libraryì˜ ëª©í‘œ**

  1. ìµœëŒ€í•œ ì‰½ê³  ë¹ ë¥´ê²Œ ì´ìš©í•˜ê¸°

  2. ê°€ëŠ¥í•œ ì›ë˜ ëª¨ë¸ê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤„ ìˆ˜ ìˆëŠ” SOTA ëª¨ë¸ì„ ì œê³µí•˜ê¸°

  3. ëª¨ë¸ì˜ ë‚´ë¶€ë¥¼ ìµœëŒ€í•œ ì¼ê´€ì ìœ¼ë¡œ ë“œëŸ¬ë‚´ê¸°

  4. ì´ ëª¨ë¸ë“¤ì„ íŒŒì¸íŠœë‹/ì—°êµ¬í•˜ê¸° ìœ„í•´ ì£¼ê´€ì ìœ¼ë¡œ ì„ íƒí•œ ìœ ë§í•œ íˆ´ë“¤ì„ í¬í•¨í•˜ê¸°

  5. PyTorchì™€ TensorFlow ê°„ì˜ ìŠ¤ìœ„ì¹­ì„ ìš©ì´í•˜ê²Œ í•˜ê¸°

     (1ê°œì˜ í”„ë ˆì„ì›Œí¬ì—ì„œ í•™ìŠµí•˜ê³  ë‹¤ë¥¸ ëª¨ë¸ì—ì„œ ì¶”ë¡ í•˜ê¸°)

- **Main Concepts**

  1. Model Class : `Transformers` ì—ì„œ ì œê³µí•˜ëŠ” ì‚¬ì „ í•™ìŠµëœ weightì™€ ì´ìš© ê°€ëŠ¥í•œ PyTorch/Keras ëª¨ë¸

     ex. `BertModel`

  2. Configuration Class : ëª¨ë¸ì„ ë§Œë“¤ê¸° ìœ„í•´ í•„ìš”í•œ ëª¨ë“  parameterë¥¼ ì €ì¥

     ex. `BertConfig`

  3. Tokenizer Class : ê° ëª¨ë¸ì„ ìœ„í•œ ë‹¨ì–´ë¥¼ ì €ì¥. ë¬¸ì¥ì„ token embeddingìœ¼ë¡œ encoding/decodingí•˜ê¸°ìœ„í•œ ë©”ì†Œë“œ.

     ex. `BertTokenizer`

  - `from_pretrained()`ë¥¼ í†µí•´ ë¼ì´ë¸ŒëŸ¬ë¦¬ë‚˜ ë¡œì»¬ì˜ ëª¨ë¸/configure/tokenizerë¥¼ ì¸ìŠ¤í„´ìŠ¤í™” í•  ìˆ˜ ìˆìŒ.
  - `save_pretrained()`ë¥¼ í†µí•´ ëª¨ë¸/configure/tokenizerë¥¼ ë¡œì»¬ì— ì €ì¥í•  ìˆ˜ ìˆìŒ.



### 4. Glossary (ìš©ì–´ ì‚¬ì „)

#### General Terms

- NLG
- NLP
- NLU
- Autoencoding Models
- Autoregressive Models
- CLM
- MLM
- Multimodal
- Pretrained Model
- RNN
- seq2seq
- token

#### Model Inputs

- Input IDs
- Attention Mask
- Token Type IDs
- Position IDs
- Feed Forward Chunking